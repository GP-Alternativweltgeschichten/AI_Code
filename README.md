# Python Code f√ºr das KI-Modell

## üìå Beschreibung
Dieser Code wurde entwickelt, um den Museumsbesuchern die M√∂glichkeit zu geben, ihre Kreativit√§t in den verschiedenen Szenarien rund um Olpe auszuleben, und ihre eigene "Alternativweltgeschichte" zu erschaffen.  
Die Codebasis umfasst Skripte zum Trainieren und Testen eines KI-Modells, zur Serververbindung mit dem Backend, zum Verarbeiten von Prompts, zur Bildgenerierung, zur Persistierung von Nutzereingaben, und ein Skript zum Testen des Servers in Python.

## üìñ Inhalt
- [Vorgehen](#-vorgehen)
- [Auswahl des KI-Modells](#-auswahl-des-ki-modells)
- [Datensatz](#%EF%B8%8F-datensatz)
- [Eigene KI-Modelle](#-eigene-ki-modelle)
- [Voraussetzungen](#%EF%B8%8F-voraussetzungen)
- [Server ausf√ºhren](#-server-ausf√ºhren)
- [Server Funktionalit√§ten](#-server-funktionalit√§ten)
- [KI-Modell trainieren](#-ki-modell-trainieren)
- [Ausblick](#-ausblick)
- [Verworfene Features](#-verworfene-features)
- [Projektstruktur](#-projektstruktur)


## üõ´ Vorgehen
Zun√§chst wurden Anforderungen an das Modell formuliert, zu finden sind diese unter [Auswahl des KI-Modells](#-auswahl-des-ki-modells).
  
Die Anforderungen resultieren aus einer gemeinsamen Diskussion √ºber das Modell und dessen Funktionalit√§ten sowie aus den Requirements.
In der Folge wurde eine Suche nach im Internet verf√ºgbaren Modellen initiiert, die den genannten Anforderungen entsprechen.
Prim√§r wurden dazu Modelle mithilfe von [Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) getestet. 
Dabei wurde erstmals evaluiert, inwiefern diese Modelle zur Generierung von Satellitenbildern bef√§higt sind.
Die Modelle selbst wurden von verschiedenen Websites wie [CivitAI](https://civitai.com/) und [HuggingFace](https://huggingface.co/) bezogen.
Modelle, die bereits in ihrer Basisvariante ad√§quate Satellitenbilder produzieren konnten, wurden anschlie√üend auf ihre F√§higkeit zum Inpaint-Verfahren untersucht. 
Durch dieses Vorgehen konnte die Anzahl der Modelle auf eine √ºberschaubare Anzahl reduziert werden.
Der entscheidende Schritt bestand nun darin zu untersuchen, ob diese Modelle durch weiteres Training in der Lage sind, spezifische Bilder f√ºr Olpe zu erstellen.
Die Mehrzahl der Modelle scheiterte an dieser Herausforderung, sei es aufgrund von zu hohen Rechenanforderungen oder der Komplexit√§t der Anforderungen.
Schlussendlich wurde ein StableDiffusion Modell selektiert und mit der [Diffusers Bibliothek](https://github.com/huggingface/diffusers) weiter trainiert.
Die Grundlage zur Wahl des Modells ist im Abschnitt [Auswahl des KI-Modells](#-auswahl-des-ki-modells) dargelegt.
Da die Ergebnisse des selbst-trainierten Modells zun√§chst nicht unseren Vorstellungen entsprachen, wurde eine zus√§tzliche Anbindung an das [Dall-E 2 Modell](https://openai.com/index/dall-e-2/) von [OpenAI](https://openai.com/) erstellt.
Dall-E 2 ist in der Lage konsistent Satellitenbilder zu erstellen, kostet aber einen geringen Preis pro Anfrage.
Das Dall-E 2 Modell hat dabei aber nicht ein eigenes Modell ersetzt, da eine kostenlose, lokale Option vorhanden sein soll.

## üéØ Auswahl des KI-Modells
Basierend auf einer ersten Literaturrecherche wurden folgende Herausforderungen bei der Erstellung eines KI-Modells identifiziert:
- Keine Open Source Modelle, die Satellitenbilder erstellen k√∂nnen 
- Keine/wenige Datens√§tze verf√ºgbar, die Satellitenbilder enthalten 
- Keine/wenige Datens√§tze verf√ºgbar, die dem Stadtbild von Olpe entsprechen 
- Teilweise sehr aufwendige Schritte zum Trainieren der Modelle 
- Teilweise hohe ben√∂tigte Rechenleistung
- Segmentierung der Satellitenbilder sehr aufwendig, da viele kleine Objekte vorhanden sind 
- Teilweise keine Unterst√ºtzung von InPaint, nur Text to Image 

Bez√ºglich der Herausforderungen musste im Laufe des Projektes ein ideales KI-Modell f√ºr unseren Anwendungsfall definiert werden.
Die folgenden Anforderungen wurden definiert:
- Open Source, kostenlos 
- Kann Satellitenbilder generieren 
- Kann lokal auf einem Rechner laufen 
- Kann von uns trainiert werden 
- Ist spezialisiert auf das Erscheinungsbild der Stadt Olpe 
- Die Erstellung eines Datensatzes und das Training sind mit angemessenem Aufwand m√∂glich
- Unterst√ºtzt InPaint 

Nach einer Reihe von Tests und zus√§tzlicher Recherche wurde schlie√ülich ein Stable Diffusion-Modell eingesetzt.
Die zuvor genannten Punkte konnten mithilfe der Diffusers-Bibliothek, die auf der [Diffusers Bibliothek](https://github.com/huggingface/diffusers) basiert, realisiert werden.
Die Diffusers Bibliothek repr√§sentiert den aktuellen Stand der Technik, ist Open-Source und bietet auch vortrainierte Modelle als Grundlage.
Dies erlaubt das Erstellen eines Datensatzes durch die Beschreibung von Bildern, anstatt durch aufwendiges Segmentieren.
Dar√ºber hinaus ist das (Weiter-)Trainieren eines Modells einfach m√∂glich.
Die Verwendung und das Weitertrainieren von vorab erstellten Modellen erm√∂glichte es, die Akkuratheit der Abbildung des Stadtbildes von Olpe zu gew√§hrleisten und zudem die Umsetzung kreativerer Anfragen zu erm√∂glichen.
Als Grundlage wurde das [Stable Diffusion v2-1 Modell](https://huggingface.co/stabilityai/stable-diffusion-2-1) von [StabilityAI](https://stability.ai/) ausgew√§hlt, welches eine umfangreiche Bilddatenbank enth√§lt.
Das trainierte Modell konnte abschlie√üend lokal mithilfe der Diffuser-Bibliothek geladen werden.
Das Modell kann in verschiedenen "Pipelines" verwendet werden, welche je nach Auswahl Text to Image, Inpaint oder weitere Aufgaben ausf√ºhren k√∂nnen.

## üó∫Ô∏è Datensatz
Zum Training von eigenen KI-Modellen wurde ein eigener Datensatz mit 540 Bildern erstellt.  
Dabei wurden Bilder mithilfe von Google Maps und anderen Satelliten-Karten erzeugt und dann beschriftet.
Es wurden haupts√§chlich Bilder aus Olpe und dessen Umgebung verwendet, damit die Bilder der KI-Modelle starke √Ñhnlichkeiten zur Karte haben.   
Der Datensatz ist auf Huggingface zu finden:   
- [Satbilder](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder) (540 Bilder, beschriftet)   

Die Bilder des Datensatzes befinden sich dabei in [/data/](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder/tree/main/data) und die zugeh√∂rigen Beschreibungen in [/metadata.csv](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder/blob/main/metadata.csv).

## ü§ñ Eigene KI-Modelle
Im laufe des Projekts wurden mehrere KI-Modelle mithilfe des Satbilder-Datensatzes trainiert.   
Das Training wurde mithilfe des [Diffusers Repository](https://github.com/huggingface/diffusers) durchgef√ºhrt.   
Eine Anleitung zum Training ist im Abschnitt [KI-Modell trainieren](#-ki-modell-trainieren), bzw. in [/model_training/model_training.ipynb](https://github.com/GP-Alternativweltgeschichten/AI_Code/blob/master/Model_Training/model_training.ipynb) zu finden.   
Alle Modelle basieren auf dem [Stable Diffusion v2-1 Modell](https://huggingface.co/stabilityai/stable-diffusion-2-1).   
Die aktuellen Modelle sind auf Huggingface zu finden:   
- [OlpeAI](https://huggingface.co/GP-Alternativweltgeschichten/OlpeAI) (10k Schritte)   
- [OlpeAI_Small](https://huggingface.co/GP-Alternativweltgeschichten/OlpeAI_Small) (700 Schritte)   

## ‚öôÔ∏è Voraussetzungen
1. Installieren Sie eine Python-Entwicklungsumgebung und ein Tool zum Verwalten von Python-Umgebungen (Optional, aber _sehr_ hilfreich).   
   In diesem Projekt wurde dazu [Pycharm](https://www.jetbrains.com/de-de/pycharm/) und [Anaconda](https://www.anaconda.com/download) verwendet.
3. Klonen Sie dieses Repository in Ihr Projektverzeichnis
4. Erstellen Sie eine Python-Umgebung mit Anaconda.  
   Dazu gehen Sie in Pycharm auf `File --> Settings --> Project: "Name des Projekts" --> Python Interpreter`.  
   W√§hlen dann Add `Interpreter --> Add Local Interpreter`.   
   Im neuen Fenster w√§hlen Sie dann `Conda Environment` und `Create new environment`.   
   Als `Environment name` w√§hlen Sie einen passenden Namen (z.B.: OlpeAI) und als `Python version: 3.10`.   
5. √ñffnen Sie nun in Pycharm ein Terminal.   
   Vor dem Dateipfad sollte nun der Name der Anaconda-Umgebung stehen.   
   Ist dies nicht der Fall, kann mit dem folgenden Befehl die Anaconda-Umgebung aktiviert werden:
   ```sh
   conda activate <Name der Umgebung>
   ```
7. Installieren Sie in dem Terminal alle relevanten Python-Bibliotheken.   
   Verwenden Sie dazu die Befehle:
   ```sh
   pip install fastapi uvicorn pydantic diffusers pillow torch requests OpenAI deep_translator transformers
   
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
   ```
   F√ºr die Pytorch Bibliotheken (zweiter Befehl) ist die im Projekt verwendete Version angegeben.   
   Ein Befehl zur Installation einer neueren Version kann auf der [Pytorch Website](https://pytorch.org/get-started/locally/) gefunden werden.   

## üöÄ Server ausf√ºhren
Erf√ºllen Sie zun√§chst die [Voraussetzungen](#-voraussetzungen).   
Starten Sie den KI-Server im Terminal (Anaconda-Umgebung muss aktiviert sein):
```sh
python ./server/inpaint_REST.py
```
Der Server l√§uft standardm√§√üig unter http://localhost:8000/

## ‚ú® Server Funktionalit√§ten
Nachfolgend werden die Funktionen der Server-Anwendung in der Reihenfolge ihrer Ausf√ºhrung beschrieben und ihre jeweilige Relevanz erl√§utert.  

Die grundlegende Server-Funktionalit√§t befindet sich in `/server/inpaint_REST.py`.
Hier wird bei Start der Anwendung zun√§chst ein KI-Modell in eine `StableDiffusionInpaintPipeline` geladen, welche die grundlegenden Inpainting-Funktionen bereitstellt.
Daraufhin wird mithilfe der [FastAPI](https://github.com/fastapi/fastapi) und [Uvicorn](https://github.com/encode/uvicorn) Bibliotheken eine REST-Schnittstelle f√ºr das Backend geladen, welche diesem das Erstellen von Inpaints erm√∂glicht.

Das Format f√ºr die Anfragen des Backends, als auch grundlegende Funktionen zum Verarbeiten des Inhalts dieser Anfragen werden in `/server/request_types.py` festgelegt.
Zum Lesen des Bildes und der Maske sind die Funktionen `get_image_as_rgb()` und `get_mask_as_rgb()` vorhanden.
Mit diesen Funktionen wird zun√§chst ein Pr√§fix von den Bildern entfernt und diese dann mithilfe der [io.BytesIO](https://docs.python.org/3/library/io.html#binary-i-o) und [Pillow](https://github.com/python-pillow/Pillow) Bibliotheken geladen.
F√ºr die Verarbeitung des Prompts ist zudem die Funktion `get_prepared_prompt()` vorhanden.
Diese stellt mit der [Deep Translator](https://github.com/nidhaloff/deep-translator) Bibliothek sicher, dass der erhaltene Prompt von Deutsch in Englisch √ºbersetzt wird, damit das KI-Modell diesen versteht.
Au√üerdem wird in der Anfrage eine Nummer, die das KI-Modell bestimmt, und eine Texttreue ben√∂tigt.
Die Modellnummer bestimmt, ob ein lokales Modell verwendet wird, oder das Inpainting von [Dall-E 2](https://openai.com/index/dall-e-2/) durchgef√ºhrt wird.
Mithilfe der Texttreue wird bestimmt, wie nah das Modell bei der Bildgeneration am Nutzer-Prompt bleibt.

Stimmt die Anfrage des Backends mit dem genannten Format √ºberein, wird in `/server/inpaint_REST.py` nun zun√§chst ein Zeitstempel zum sp√§teren Persistieren der Anfragen erstellt.
Daraufhin wird √ºberpr√ºft, ob die Anfrage eine valide Maske enth√§lt.
Ist dies nicht der Fall, so wird das Bild, das ver√§ndert werden sollte, mit der `send_image_as_png()`-Funktion, ohne √Ñnderungen zur√ºck an das Backend geschickt.
Ist eine valide Maske vorhanden, werden zun√§chst alle Daten der Anfrage geladen.
Daraufhin wird √ºberpr√ºft, welches Modell gew√§hlt wurde.
Dabei steht `0` f√ºr das lokale Modell und `1` f√ºr das Dall-E 2 Modell.
Wird das lokale Modell verwendet, wird zun√§chst der Prompt √ºbersetzt und dann mithilfe der `get_enhanced_prompt()`-Funktion aus `/server/prompt_engineering.py` verbessert.

In der `get_enhanced_prompt()`-Funktion werden zun√§chst alle Satzzeichen aus dem Prompt entfernt und die W√∂rter in Kleinbuchstaben ge√§ndert.
Daraufhin wird f√ºr jedes Wort √ºberpr√ºft, ob dieses in einem angegebenen Prompt-Lexikon vorkommt.
Ist dies der Fall, so wird f√ºr jedes dieser Worte ein bestimmter (komplexerer/kreativerer) Begriff zum originalen Prompt hinzugef√ºgt, um das Ergebnis der Bildgenerierung zu verbessern.
Im finalen Stand des Projekts sind in dem Prompt-Lexikon nur √úbersetzungen von wichtigen Begriffen vorhanden, falls die automatische √úbersetzung fehlschl√§gt, da das finale Modell auch mit simplem Prompts gut zurecht kommt.
Das tats√§chliche Verbessern der Prompts kann jedoch durch neue Eintr√§ge im Prompt-Lexikon wieder hinzugef√ºgt werden.

Nach der Verbesserung des Prompts wird beim lokalen Modell schlie√ülich das Inpainting mit den gegebenen Parametern in der Funktion `inpaint_image_with_custom_model()` aus `/server/image_processing.py` gestartet.
Dabei werden zun√§chst das Bild und die Maske mit der `crop_masked_region()`-Funktion auf eine gegebene Gr√∂√üe (hier: 768x768 Pixel) zugeschnitten, damit das Modell diese verarbeiten kann.
Dabei wird sichergestellt, dass nicht nur der vom Nutzer markierte Bereich weitergegeben wird, sondern auch ein kleiner Bereich au√üerhalb der Markierung, damit die √úberg√§nge zu den nicht ver√§nderten Bereichen akkurater sind.
Daraufhin wird mit der `convert_mask()`-Funktion die Maske f√ºr das Modell so angepasst, dass Bereiche, die ver√§ndert werden sollen, transparent (oder wei√ü) sind und Bereiche, die nicht ver√§ndert werden sollen wei√ü.
Schlie√ülich wird das tats√§chliche Inpainting mit dem Modell durchgef√ºhrt.
Hierbei werden die √ºbergebenen Parameter verwendet und zudem die St√§rke und die Anzahl an Inferenz-Schritten angegeben.
Die St√§rke und Inferenz-Schritte wurden nach mehreren Tests auf einen passenden Wert festgelegt und beeinflussen, wie stark sich das originale Bilder ver√§ndert.
Diese Parameter sind nicht vom Nutzer einzugeben, da mit den festen Werten gute Ergebnisse entstehen und weitere Parameter die Nutzer verwirren k√∂nnte.
Nachdem das Bild generiert wurde, wird dieses mithilfe der `insert_inpainted_region()`-Funktion wieder auf die originale Gr√∂√üe skaliert und schlie√ülich zur√ºckgegeben.
Damit ist das Inpainting mit dem lokalen Modell abgeschlossen.

Wird das Dall-E 2 Modell gew√§hlt, so wird die `inpaint_image_with_dalle()`-Funktion verwendet.
In dieser wird zun√§chst ein OpenAI-Client mithilfe der [OpenAI](https://github.com/openai/openai-python) Bibliothek erstellt.
Daraufhin wird das Bild und die Maske, wie beim lokalen Modell, mit der `crop_masked_region()` und `convert_mask()` angepasst.
Dann werden diese tempor√§r f√ºr den Client gespeichert mit der `save_temp_image()`-Funktion.
Nun werden einige grundlegende Parameter f√ºr die Anfrage an das Modell festgelegt.
Mithilfe der tempor√§r gespeicherten Bilder und den Parametern wird dann die Anfrage an das Modell gesendet.
Wird ein valides Ergebnis zur√ºckgeliefert, so werden die tempor√§ren Bilder gel√∂scht, das generierte Bild aus der Antwort extrahiert, und schlie√ülich das Bild wieder mit der `insert_inpainted_region()`-Funktion skaliert und zur√ºckgegeben.

Nach der Bildgenerierung von einem der Modelle, werden die Anfrage und das Ergebnis mit den Funktionen `save_income()` und `save_result()` aus `/server/persistence.py` persistiert.
F√ºr die Anfrage wird das grundlegende Bild und die Maske mit Zeitstempel in `/persistence/img/input/`, und der Prompt mit dem gew√§hlten Modell, der Texttreue und Zeitstempel in `/persistence/prompt/prompts.txt` gespeichert.
F√ºr die Ergebnisse wird nur das generierte Bild mit Zeitstempel in `/persistence/img/result/` gespeichert.

Zum Abschluss wird das Ergebnis des jeweiligen Modells mit der `send_image_as_png()`-Funktion an das Backend zur√ºckgesendet.


## üì• KI-Modell trainieren
1. Erf√ºllen Sie zun√§chst die [Voraussetzungen](#-voraussetzungen).
2. √ñffnen Sie nun [/model_training/model_training.ipynb](https://github.com/GP-Alternativweltgeschichten/AI_Code/blob/master/Model_Training/model_training.ipynb).
3. F√ºhren Sie der Reihe nach alle Zellen des Jupyter Notebooks aus:
4. **Requirements:**
   1. Zun√§chst werden weitere, wichtige Bibliotheken installiert.
   2. Dann wird der im Projekt erstellte Datensatz ([Satbilder](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder)) geladen.
   3. Nun werden die n√∂tigen Bibliotheken geladen.
5. **Training the model:**   
   1. Dieser Schritt kann entweder direkt im Jupyter Notebook durchgef√ºhrt werden oder im Terminal.  
      Aus eigener Erfahrung ist das Ausf√ºhren des Befehls im Terminal empfehlenswert, da PyCharm nach einiger Zeit abst√ºrzen kann.
      Achten Sie beim Ausf√ºhren im Terminal darauf, dass die Anaconda-Umgebung aktiviert ist.
   2. Passen Sie den Befehl auf die eigenen Bed√ºrfnisse an.  
      Die wichtigsten Parameter sind dabei `train_batch_size`, `gradient_accumulation_steps` und `max_train_steps`.
      - **`train_batch_size`** bestimmt, wie viele Bilder gleichzeitig verarbeitet werden und hat den gr√∂√üten Einfluss auf den VRAM-Verbrauch.  
      - **`gradient_accumulation_steps`** gibt an, wie viele Schritte Gradienten gespeichert werden, bevor ein Update des Modells erfolgt.  
        Ein h√∂herer Wert kann helfen, mit kleineren `train_batch_size`-Werten zu arbeiten, um VRAM zu sparen.  
      - **`max_train_steps`** definiert die maximale Anzahl an Trainingsschritten. Ein h√∂herer Wert f√ºhrt zu einer l√§ngeren Trainingszeit, _kann_ aber zu besseren Ergebnissen f√ºhren.
      
      Zudem kann das Modell, das als Grundlage verwendet wird, in `pretrained_model_name_or_path` angegeben werden.  
      Dabei kann entweder ein lokaler Pfad angegeben werden oder der Name des Modells auf [Hugging Face](https://huggingface.co/).  
   3. F√ºhren Sie die Zelle, bzw. den Befehl im Terminal aus. Das Training sollte nun automatisch laufen.  
      Das Modell wird schlie√ülich in `output_dir` gespeichert.
6. **Testing the model:**
   1. Zun√§chst wird das Modell in eine `StableDiffusionInpaintPipeline` geladen, die alle n√∂tigen Funktionen bereitstellt.  
      Passen Sie dabei den Pfad (erster Parameter) auf den Namen des Modells an.
   2. Diese Pipeline wird dann in die GPU geladen.  
      Verwenden Sie keine GPU, so kann auch `cpu`, statt `cuda` angegeben werden.
   3. Nun werden die beiden grundlegenden Bilder geladen und angezeigt.  
      Der Bildausschnitt, der Ver√§nderungen erhalten soll (`initial_image`) und die Maske (`mask_image`), die bestimmt wo die Ver√§nderungen stattfinden.
   4. Danach k√∂nnen mit dem Modell Bilder generiert werden.  
      Dazu muss ein Prompt angegeben werden und dieser, zusammen mit den Bildern und weiteren Parametern an die Pipeline gegeben werden.
      Die Erkl√§rung der einzelnen Parameter finden Sie in (//TODO).
   5. Schlie√ülich werden die Bilder angezeigt. Es empfiehlt sich mehrere Bilder zu erzeugen, um die Konsistenz der Generierung zu √ºberpr√ºfen.



## üî≠ Ausblick
Nach umfassender Analyse und Evaluation wurde festgestellt, dass das KI-Modell in seiner gegenw√§rtigen Konfiguration noch nicht die gew√ºnschte Trainingsreife aufweist. F√ºr die Zukunft ist demnach die weitere Optimierung des Modells auf Basis einer gr√∂√üeren Anzahl noch detaillierter beschrifteter Bilder vorgesehen.
Dar√ºber hinaus ist die Verwendung eines alternativen Modells f√ºr die gleiche Aufgabe eine m√∂gliche zuk√ºnftige Entwicklung, sollte sich dieses als √ºberlegen erweisen.
Dar√ºber hinaus sollte, unter Beibehaltung der Topografie, eine Integration der Topografie in das KI-Modell erwogen werden. Daf√ºr m√ºsste das Modell eventuell neu trainiert werden oder gar ein anderes genutzt werden. Die Integration von Fl√ºssen, die den Berg hinabflie√üen oder Seen, die auf reaktive Weise auf Bergen durch das Modell generiert werden, ist eine vielversprechende M√∂glichkeit, die durch die Einbindung der Topografie in das KI-Modell realisiert werden k√∂nnte. 
F√ºr die Zukunft sehen wir auch die M√∂glichkeit, andere Modelle als LORAS zu verwenden, um die Stadt in verschiedenen Stilen wie Mittelalter, Japanisch, Wilder Westen usw. darzustellen.
Es ist jedoch darauf hinzuweisen, dass auch ein g√§nzlich divergierendes Vorgehen in der Zukunft nicht ausgeschlossen werden kann. Es existieren bereits √§hnliche Projekte, wie das von Nvidia ([Nvidia Canvas](https://support.nvidia.eu/hc/de/articles/360017442939-NVIDIA-CANVAS)), die demonstrieren, dass die zugrunde liegende Problemstellung nicht ausschlie√ülich durch die vorgeschlagene L√∂sung gel√∂st werden kann. Sollte sich f√ºr ein alternatives Vorgehen entschieden werden, w√§re eine entsprechende Anpassung des Modells erforderlich.

## üöÆ Verworfene Features 
Die Mehrzahl der Features, die im Rahmen des Projektes konzipiert wurden, wurde in der vorliegenden Form realisiert. Dies ist auf die Tatsache zur√ºckzuf√ºhren, dass sich die Projektgruppe bei der Entwicklung des KI-Modells nicht mit utopischen Zielen und Features befasst hat. 
Einige wenige Features wurden jedoch verworfen.
Ein Beispiel ist die M√∂glichkeit, die Text-to-Image-Funktion des Modells zu nutzen.
Diese wurde zun√§chst implementiert, dann jedoch durch die Inpaint-Funktion ersetzt. Dieser Schritt wurde unternommen, da die Notwendigkeit bzw. der Nutzen dieser Funktion f√ºr die G√§ste nicht l√§nger als gegeben angesehen wurde. Sollte eine Revision dieses Vorgehens in Betracht gezogen werden, w√§re eine erneute Implementierung dieses Features eine m√∂gliche Konsequenz.
Zudem wurde zu Projektbeginn die Idee diskutiert, die Karte in einer 3D-Ansicht zu pr√§sentieren, in der das Modell darauf trainiert werden oder ein anderes Modell ben√∂tigt werden w√ºrde. Diese Idee wurde jedoch aus Gr√ºnden der Projektgr√∂√üe verworfen. Dar√ºber hinaus wurden weitere kleinere Features, die √§hnliche Gr√ºnde wie Text-to-Image aufwiesen, entfernt.
Auch die Idee, die Stadt mit Hilfe von KI in verschiedenen Stilen wie Mittelalter oder wilder Westen zu gestalten, wurde zun√§chst zur√ºckgestellt und aus Zeitgr√ºnden erstmals in den Ausblick f√ºr die zuk√ºnftige Entwicklung aufgenommen.

## üìÇ Projektstruktur
```
/client                     # Verzeichnis f√ºr den Test-Client
  /rest_client.py           # Python-Datei f√ºr den Test-Client
/model_training             # Verzeichnis f√ºr das Trainieren und Testen von KI-Modellen
  /test_images              # Enth√§lt zwei Bilder zum Testen der Inpaint-Funktion von KI-Modellen
  /model_training.ipynb     # Pyhton-Notebook mit allen relevanten Anforderungen und Skripten zum Trainieren und Testen von KI-Modellen
/persistence                # Verzeichnis zum Aufzeichnen der Prompts und Ausgew√§hlten Bereiche der Nutzer
  /img                      # Verzeichnis zum Speichern der Eingabe-Bilder und generierten Ausgabe-Bilder
  /prompt                   # Verzeichnis zum Speichern der textuellen Prompts
/server                     # Verzeichnis f√ºr die Server-Dateien
  /image_processing.py      # Python-Datei mit Funktionen zur Verarbeitung der Bilder und dem Ausf√ºhren des Inpaintings
  /inpaint_REST.py          # Python-Datei f√ºr den Server
  /persistence.py           # Python-Datei zur Persistierung der Nutzereingaben und Ausgaben des KI-Modells
  /prompt_engineering.py    # Python-Datei mit einer Funktion zum Verbessern von Prompts
  /request_types.py         # Python-Datei, die die Form des Requests festlegt und Funktionen zum Verarbeiten dessen bereitstellt.
```

--- 
