# Python Code f√ºr das KI-Modell

## üìå Beschreibung
Dieser Code wurde entwickelt, um den Museumsbesuchern die M√∂glichkeit zu geben, ihre Kreativit√§t in den verschiedenen Szenarien rund um Olpe auszuleben, und ihre eigene "Alternativweltgeschichte" zu erschaffen.  
Die Codebasis umfasst Skripte zum Trainieren und Testen eines KI-Modells, zur Serververbindung mit dem Backend, zum Verarbeiten von Prompts, zur Bildgenerierung, zur Persistierung von Nutzereingaben, und ein Skript zum Testen des Servers in Python.

## üìñ Inhalt
- [Vorgehen](#-vorgehen)
- [Auswahl des KI-Modells](#-auswahl-des-ki-modells)
- [Datensatz](#%EF%B8%8F-datensatz)
- [Eigene KI-Modelle](#-eigene-ki-modelle)
- [Voraussetzungen](#%EF%B8%8F-voraussetzungen)
- [Server ausf√ºhren](#-server-ausf√ºhren)
- [Server Funktionalit√§ten](#-server-funktionalit√§ten)
- [KI-Prompting](#-ki-prompting)
- [KI-Modell trainieren](#-ki-modell-trainieren)
- [Ausblick](#-ausblick)
- [Verworfene Features](#-verworfene-features)
- [Projektstruktur](#-projektstruktur)


## üõ´ Vorgehen
Zun√§chst wurden Anforderungen an das Modell formuliert, zu finden sind diese unter [Auswahl des KI-Modells](#-auswahl-des-ki-modells).
  
Die Anforderungen resultieren aus einer gemeinsamen Diskussion √ºber das Modell und dessen Funktionalit√§ten sowie aus den Requirements.
In der Folge wurde eine Suche nach im Internet verf√ºgbaren Modellen initiiert, die den genannten Anforderungen entsprechen.
Prim√§r wurden dazu Modelle mithilfe von [Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) getestet. 
Dabei wurde erstmals evaluiert, inwiefern diese Modelle zur Generierung von Satellitenbildern bef√§higt sind.
Die Modelle selbst wurden von verschiedenen Websites wie [CivitAI](https://civitai.com/) und [HuggingFace](https://huggingface.co/) bezogen.
Modelle, die bereits in ihrer Basisvariante ad√§quate Satellitenbilder produzieren konnten, wurden anschlie√üend auf ihre F√§higkeit zum Inpaint-Verfahren untersucht. 
Durch dieses Vorgehen konnte die Anzahl der Modelle auf eine √ºberschaubare Anzahl reduziert werden.
Der entscheidende Schritt bestand nun darin zu untersuchen, ob diese Modelle durch weiteres Training in der Lage sind, spezifische Bilder f√ºr Olpe zu erstellen.
Die Mehrzahl der Modelle scheiterte an dieser Herausforderung, sei es aufgrund von zu hohen Rechenanforderungen oder der Komplexit√§t der Anforderungen.
Schlussendlich wurde ein StableDiffusion Modell selektiert und mit der [Diffusers Bibliothek](https://github.com/huggingface/diffusers) weiter trainiert.
Die Grundlage zur Wahl des Modells ist im Abschnitt [Auswahl des KI-Modells](#-auswahl-des-ki-modells) dargelegt.
Da die Ergebnisse des selbst-trainierten Modells zun√§chst nicht unseren Vorstellungen entsprachen, wurde eine zus√§tzliche Anbindung an das [Dall-E 2 Modell](https://openai.com/index/dall-e-2/) von [OpenAI](https://openai.com/) erstellt.
Dall-E 2 ist in der Lage konsistent Satellitenbilder zu erstellen, kostet aber einen geringen Preis pro Anfrage.
Das Dall-E 2 Modell hat dabei aber nicht ein eigenes Modell ersetzt, da eine kostenlose, lokale Option vorhanden sein soll.

## üéØ Auswahl des KI-Modells
Basierend auf einer ersten Literaturrecherche wurden folgende Herausforderungen bei der Erstellung eines KI-Modells identifiziert:
- Keine Open Source Modelle, die Satellitenbilder erstellen k√∂nnen 
- Keine/wenige Datens√§tze verf√ºgbar, die Satellitenbilder enthalten 
- Keine/wenige Datens√§tze verf√ºgbar, die dem Stadtbild von Olpe entsprechen 
- Teilweise sehr aufwendige Schritte zum Trainieren der Modelle 
- Teilweise hohe ben√∂tigte Rechenleistung
- Segmentierung der Satellitenbilder sehr aufwendig, da viele kleine Objekte vorhanden sind 
- Teilweise keine Unterst√ºtzung von InPaint, nur Text to Image 

Bez√ºglich der Herausforderungen musste im Laufe des Projektes ein ideales KI-Modell f√ºr unseren Anwendungsfall definiert werden.
Die folgenden Anforderungen wurden definiert:
- Open Source, kostenlos 
- Kann Satellitenbilder generieren 
- Kann lokal auf einem Rechner laufen 
- Kann von uns trainiert werden 
- Ist spezialisiert auf das Erscheinungsbild der Stadt Olpe 
- Die Erstellung eines Datensatzes und das Training sind mit angemessenem Aufwand m√∂glich
- Unterst√ºtzt InPaint 

Nach einer Reihe von Tests und zus√§tzlicher Recherche wurde schlie√ülich ein Stable Diffusion-Modell eingesetzt.
Die zuvor genannten Punkte konnten mithilfe der Diffusers-Bibliothek, die auf der [Diffusers Bibliothek](https://github.com/huggingface/diffusers) basiert, realisiert werden.
Die Diffusers Bibliothek repr√§sentiert den aktuellen Stand der Technik, ist Open-Source und bietet auch vortrainierte Modelle als Grundlage.
Dies erlaubt das Erstellen eines Datensatzes durch die Beschreibung von Bildern, anstatt durch aufwendiges Segmentieren.
Dar√ºber hinaus ist das (Weiter-)Trainieren eines Modells einfach m√∂glich.
Die Verwendung und das Weitertrainieren von vorab erstellten Modellen erm√∂glichte es, die Akkuratheit der Abbildung des Stadtbildes von Olpe zu gew√§hrleisten und zudem die Umsetzung kreativerer Anfragen zu erm√∂glichen.
Als Grundlage wurde das [Stable Diffusion v2-1 Modell](https://huggingface.co/stabilityai/stable-diffusion-2-1) von [StabilityAI](https://stability.ai/) ausgew√§hlt, welches eine umfangreiche Bilddatenbank enth√§lt.
Das trainierte Modell konnte abschlie√üend lokal mithilfe der Diffuser-Bibliothek geladen werden.
Das Modell kann in verschiedenen "Pipelines" verwendet werden, welche je nach Auswahl Text to Image, Inpaint oder weitere Aufgaben ausf√ºhren k√∂nnen.

## üó∫Ô∏è Datensatz
Zum Training von eigenen KI-Modellen wurde ein eigener Datensatz mit 540 Bildern erstellt.  
Dabei wurden Bilder mithilfe von Google Maps und anderen Satelliten-Karten erzeugt und dann beschriftet.
Es wurden haupts√§chlich Bilder aus Olpe und dessen Umgebung verwendet, damit die Bilder der KI-Modelle starke √Ñhnlichkeiten zur Karte haben.   
Der Datensatz ist auf Huggingface zu finden:   
- [Satbilder](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder) (540 Bilder, beschriftet)   

Die Bilder des Datensatzes befinden sich dabei in [/data/](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder/tree/main/data) und die zugeh√∂rigen Beschreibungen in [/metadata.csv](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder/blob/main/metadata.csv).

## ü§ñ Eigene KI-Modelle
Im laufe des Projekts wurden mehrere KI-Modelle mithilfe des Satbilder-Datensatzes trainiert.   
Das Training wurde mithilfe des [Diffusers Repository](https://github.com/huggingface/diffusers) durchgef√ºhrt.   
Eine Anleitung zum Training ist im Abschnitt [KI-Modell trainieren](#-ki-modell-trainieren), bzw. in [/model_training/model_training.ipynb](https://github.com/GP-Alternativweltgeschichten/AI_Code/blob/master/Model_Training/model_training.ipynb) zu finden.   
Alle Modelle basieren auf dem [Stable Diffusion v2-1 Modell](https://huggingface.co/stabilityai/stable-diffusion-2-1).   
Die aktuellen Modelle sind auf Huggingface zu finden:   
- [OlpeAI](https://huggingface.co/GP-Alternativweltgeschichten/OlpeAI) (10k Schritte)   
- [OlpeAI_Small](https://huggingface.co/GP-Alternativweltgeschichten/OlpeAI_Small) (700 Schritte)   

## ‚öôÔ∏è Voraussetzungen
1. Installieren Sie eine Python-Entwicklungsumgebung und ein Tool zum Verwalten von Python-Umgebungen (Optional, aber _sehr_ hilfreich).   
   In diesem Projekt wurde dazu [Pycharm](https://www.jetbrains.com/de-de/pycharm/) und [Anaconda](https://www.anaconda.com/download) verwendet.
3. Klonen Sie dieses Repository in Ihr Projektverzeichnis
4. Erstellen Sie eine Python-Umgebung mit Anaconda.  
   Dazu gehen Sie in Pycharm auf `File --> Settings --> Project: "Name des Projekts" --> Python Interpreter`.  
   W√§hlen dann Add `Interpreter --> Add Local Interpreter`.   
   Im neuen Fenster w√§hlen Sie dann `Conda Environment` und `Create new environment`.   
   Als `Environment name` w√§hlen Sie einen passenden Namen (z.B.: OlpeAI) und als `Python version: 3.10`.   
5. √ñffnen Sie nun in Pycharm ein Terminal.   
   Vor dem Dateipfad sollte nun der Name der Anaconda-Umgebung stehen.   
   Ist dies nicht der Fall, kann mit dem folgenden Befehl die Anaconda-Umgebung aktiviert werden:
   ```sh
   conda activate <Name der Umgebung>
   ```
7. Installieren Sie in dem Terminal alle relevanten Python-Bibliotheken.   
   Verwenden Sie dazu die Befehle:
   ```sh
   pip install fastapi uvicorn pydantic diffusers pillow torch requests OpenAI deep_translator transformers
   
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
   ```
   F√ºr die Pytorch Bibliotheken (zweiter Befehl) ist die im Projekt verwendete Version angegeben.   
   Ein Befehl zur Installation einer neueren Version kann auf der [Pytorch Website](https://pytorch.org/get-started/locally/) gefunden werden.   

## üöÄ Server ausf√ºhren
Erf√ºllen Sie zun√§chst die [Voraussetzungen](#-voraussetzungen).   
Starten Sie den KI-Server im Terminal (Anaconda-Umgebung muss aktiviert sein):
```sh
python ./server/inpaint_REST.py
```
Der Server l√§uft standardm√§√üig unter http://localhost:8000/

## ‚ú® Server Funktionalit√§ten
Welche Funktionen haben wir? Warum? Reihenfolge der Ausf√ºhrung? Was kommt rein/Was geht raus? 

## üìù KI-Prompting
Parameter? Was m√ºssen wir beachten?

## üì• KI-Modell trainieren
1. Erf√ºllen Sie zun√§chst die [Voraussetzungen](#-voraussetzungen).
2. √ñffnen Sie nun [/model_training/model_training.ipynb](https://github.com/GP-Alternativweltgeschichten/AI_Code/blob/master/Model_Training/model_training.ipynb).
3. F√ºhren Sie der Reihe nach alle Zellen des Jupyter Notebooks aus:
4. **Requirements:**
   1. Zun√§chst werden weitere, wichtige Bibliotheken installiert.
   2. Dann wird der im Projekt erstellte Datensatz ([Satbilder](https://huggingface.co/datasets/GP-Alternativweltgeschichten/Satbilder)) geladen.
   3. Nun werden die n√∂tigen Bibliotheken geladen.
5. **Training the model:**   
   1. Dieser Schritt kann entweder direkt im Jupyter Notebook durchgef√ºhrt werden oder im Terminal.  
      Aus eigener Erfahrung ist das Ausf√ºhren des Befehls im Terminal empfehlenswert, da PyCharm nach einiger Zeit abst√ºrzen kann.
      Achten Sie beim Ausf√ºhren im Terminal darauf, dass die Anaconda-Umgebung aktiviert ist.
   2. Passen Sie den Befehl auf die eigenen Bed√ºrfnisse an.  
      Die wichtigsten Parameter sind dabei `train_batch_size`, `gradient_accumulation_steps` und `max_train_steps`.
      - **`train_batch_size`** bestimmt, wie viele Bilder gleichzeitig verarbeitet werden und hat den gr√∂√üten Einfluss auf den VRAM-Verbrauch.  
      - **`gradient_accumulation_steps`** gibt an, wie viele Schritte Gradienten gespeichert werden, bevor ein Update des Modells erfolgt.  
        Ein h√∂herer Wert kann helfen, mit kleineren `train_batch_size`-Werten zu arbeiten, um VRAM zu sparen.  
      - **`max_train_steps`** definiert die maximale Anzahl an Trainingsschritten. Ein h√∂herer Wert f√ºhrt zu einer l√§ngeren Trainingszeit, _kann_ aber zu besseren Ergebnissen f√ºhren.
      
      Zudem kann das Modell, das als Grundlage verwendet wird, in `pretrained_model_name_or_path` angegeben werden.  
      Dabei kann entweder ein lokaler Pfad angegeben werden oder der Name des Modells auf [Hugging Face](https://huggingface.co/).  
   3. F√ºhren Sie die Zelle, bzw. den Befehl im Terminal aus. Das Training sollte nun automatisch laufen.  
      Das Modell wird schlie√ülich in `output_dir` gespeichert.
6. **Testing the model:**
   1. Zun√§chst wird das Modell in eine `StableDiffusionInpaintPipeline` geladen, die alle n√∂tigen Funktionen bereitstellt.  
      Passen Sie dabei den Pfad (erster Parameter) auf den Namen des Modells an.
   2. Diese Pipeline wird dann in die GPU geladen.  
      Verwenden Sie keine GPU, so kann auch `cpu`, statt `cuda` angegeben werden.
   3. Nun werden die beiden grundlegenden Bilder geladen und angezeigt.  
      Der Bildausschnitt, der Ver√§nderungen erhalten soll (`initial_image`) und die Maske (`mask_image`), die bestimmt wo die Ver√§nderungen stattfinden.
   4. Danach k√∂nnen mit dem Modell Bilder generiert werden.  
      Dazu muss ein Prompt angegeben werden und dieser, zusammen mit den Bildern und weiteren Parametern an die Pipeline gegeben werden.
      Die Erkl√§rung der einzelnen Parameter finden Sie in (//TODO).
   5. Schlie√ülich werden die Bilder angezeigt. Es empfiehlt sich mehrere Bilder zu erzeugen, um die Konsistenz der Generierung zu √ºberpr√ºfen.



## üî≠ Ausblick
Nach umfassender Analyse und Evaluation wurde festgestellt, dass das KI-Modell in seiner gegenw√§rtigen Konfiguration noch nicht die gew√ºnschte Trainingsreife aufweist. F√ºr die Zukunft ist demnach die weitere Optimierung des Modells auf Basis einer gr√∂√üeren Anzahl noch detaillierter beschrifteter Bilder vorgesehen.
Dar√ºber hinaus ist die Verwendung eines alternativen Modells f√ºr die gleiche Aufgabe eine m√∂gliche zuk√ºnftige Entwicklung, sollte sich dieses als √ºberlegen erweisen.
Dar√ºber hinaus sollte, unter Beibehaltung der Topografie, eine Integration der Topografie in das KI-Modell erwogen werden. Daf√ºr m√ºsste das Modell eventuell neu trainiert werden oder gar ein anderes genutzt werden. Die Integration von Fl√ºssen, die den Berg hinabflie√üen oder Seen, die auf reaktive Weise auf Bergen durch das Modell generiert werden, ist eine vielversprechende M√∂glichkeit, die durch die Einbindung der Topografie in das KI-Modell realisiert werden k√∂nnte. 
F√ºr die Zukunft sehen wir auch die M√∂glichkeit, andere Modelle als LORAS zu verwenden, um die Stadt in verschiedenen Stilen wie Mittelalter, Japanisch, Wilder Westen usw. darzustellen.
Es ist jedoch darauf hinzuweisen, dass auch ein g√§nzlich divergierendes Vorgehen in der Zukunft nicht ausgeschlossen werden kann. Es existieren bereits √§hnliche Projekte, wie das von Nvidia ([Nvidia Canvas](https://support.nvidia.eu/hc/de/articles/360017442939-NVIDIA-CANVAS)), die demonstrieren, dass die zugrunde liegende Problemstellung nicht ausschlie√ülich durch die vorgeschlagene L√∂sung gel√∂st werden kann. Sollte sich f√ºr ein alternatives Vorgehen entschieden werden, w√§re eine entsprechende Anpassung des Modells erforderlich.

## üöÆ Verworfene Features 
Die Mehrzahl der Features, die im Rahmen des Projektes konzipiert wurden, wurde in der vorliegenden Form realisiert. Dies ist auf die Tatsache zur√ºckzuf√ºhren, dass sich die Projektgruppe bei der Entwicklung des KI-Modells nicht mit utopischen Zielen und Features befasst hat. 
Einige wenige Features wurden jedoch verworfen.
Ein Beispiel ist die M√∂glichkeit, die Text-to-Image-Funktion des Modells zu nutzen.
Diese wurde zun√§chst implementiert, dann jedoch durch die Inpaint-Funktion ersetzt. Dieser Schritt wurde unternommen, da die Notwendigkeit bzw. der Nutzen dieser Funktion f√ºr die G√§ste nicht l√§nger als gegeben angesehen wurde. Sollte eine Revision dieses Vorgehens in Betracht gezogen werden, w√§re eine erneute Implementierung dieses Features eine m√∂gliche Konsequenz.
Zudem wurde zu Projektbeginn die Idee diskutiert, die Karte in einer 3D-Ansicht zu pr√§sentieren, in der das Modell darauf trainiert werden oder ein anderes Modell ben√∂tigt werden w√ºrde. Diese Idee wurde jedoch aus Gr√ºnden der Projektgr√∂√üe verworfen. Dar√ºber hinaus wurden weitere kleinere Features, die √§hnliche Gr√ºnde wie Text-to-Image aufwiesen, entfernt.
Auch die Idee, die Stadt mit Hilfe von KI in verschiedenen Stilen wie Mittelalter oder wilder Westen zu gestalten, wurde zun√§chst zur√ºckgestellt und aus Zeitgr√ºnden erstmals in den Ausblick f√ºr die zuk√ºnftige Entwicklung aufgenommen.

## üìÇ Projektstruktur
```
/client                     # Verzeichnis f√ºr den Test-Client
  /rest_client.py           # Python-Datei f√ºr den Test-Client
/model_training             # Verzeichnis f√ºr das Trainieren und Testen von KI-Modellen
  /test_images              # Enth√§lt zwei Bilder zum Testen der Inpaint-Funktion von KI-Modellen
  /model_training.ipynb     # Pyhton-Notebook mit allen relevanten Anforderungen und Skripten zum Trainieren und Testen von KI-Modellen
/persistence                # Verzeichnis zum Aufzeichnen der Prompts und Ausgew√§hlten Bereiche der Nutzer
  /img                      # Verzeichnis zum Speichern der Eingabe-Bilder und generierten Ausgabe-Bilder
  /prompt                   # Verzeichnis zum Speichern der textuellen Prompts
/server                     # Verzeichnis f√ºr die Server-Dateien
  /image_processing.py      # Python-Datei mit Funktionen zur Verarbeitung der Bilder und dem Ausf√ºhren des Inpaintings
  /inpaint_REST.py          # Python-Datei f√ºr den Server
  /persistence.py           # Python-Datei zur Persistierung der Nutzereingaben und Ausgaben des KI-Modells
  /prompt_engineering.py    # Python-Datei mit einer Funktion zum Verbessern von Prompts
  /request_types.py         # Python-Datei, die die Form des Requests festlegt und Funktionen zum Verarbeiten dessen bereitstellt.
```

--- 
